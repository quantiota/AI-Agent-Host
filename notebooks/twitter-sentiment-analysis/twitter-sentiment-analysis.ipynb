{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea025b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "import psycopg2\n",
    "import psycopg2.pool\n",
    "import psycopg2.extras\n",
    "import threading\n",
    "import signal\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "QDB_PG_NAME = os.getenv('QDB_PG_NAME', 'qdb')\n",
    "QDB_PG_USER = os.getenv('QDB_PG_USER', 'admin')\n",
    "QDB_PG_PASSWORD = os.getenv('QDB_PG_PASSWORD', 'quest')\n",
    "QDB_PG_HOST = os.getenv('QDB_PG_HOST', 'localhost')  # Changed from docker_host_ip_address\n",
    "QDB_PG_PORT = os.getenv('QDB_PG_PORT', '8812')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb96424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Constants\n",
    "BATCH_SIZE = 1000  # Process tweets in batches\n",
    "MAX_WORKERS = 4    # Number of parallel processing threads\n",
    "RETRY_ATTEMPTS = 3 # Number of retry attempts for failed operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ca32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('twitter_sentiment.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterSentimentProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Twitter Sentiment Processor\"\"\"\n",
    "        self.connection_pool = None\n",
    "        self.setup_connection_pool()\n",
    "        self.processed_count = 0\n",
    "        self.failed_count = 0\n",
    "        self.start_time = None\n",
    "        \n",
    "    def setup_connection_pool(self):\n",
    "        \"\"\"Initialize database connection pool\"\"\"\n",
    "        try:\n",
    "            self.connection_pool = psycopg2.pool.SimpleConnectionPool(\n",
    "                1, 10,\n",
    "                dbname=QDB_PG_NAME,\n",
    "                user=QDB_PG_USER,\n",
    "                password=QDB_PG_PASSWORD,\n",
    "                host=QDB_PG_HOST,\n",
    "                port=QDB_PG_PORT\n",
    "            )\n",
    "            logger.info(\"‚úÖ Database connection pool initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing connection pool: {e}\")\n",
    "            raise\n",
    "\n",
    "    def download_dataset(self, url: str, filename: str) -> bool:\n",
    "        \"\"\"Download dataset with improved error handling and progress tracking\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting download from: {url}\")\n",
    "            \n",
    "            # Add headers to avoid being blocked\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, stream=True, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get file size for progress tracking\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            downloaded_size = 0\n",
    "            \n",
    "            with open(filename, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        downloaded_size += len(chunk)\n",
    "                        \n",
    "                        # Show progress every 1MB\n",
    "                        if downloaded_size % (1024 * 1024) == 0:\n",
    "                            if total_size > 0:\n",
    "                                progress = (downloaded_size / total_size) * 100\n",
    "                                logger.info(f\"Download progress: {progress:.1f}%\")\n",
    "            \n",
    "            logger.info(f\"Dataset downloaded successfully as {filename}\")\n",
    "            logger.info(f\"File size: {downloaded_size / (1024*1024):.2f} MB\")\n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Network error downloading dataset: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading dataset: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            return False\n",
    "\n",
    "    def create_questdb_table(self) -> bool:\n",
    "        \"\"\"Create QuestDB table with improved schema\"\"\"\n",
    "        conn = self.connection_pool.getconn()\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                # Drop table if exists (for testing - remove in production)\n",
    "                # cur.execute(\"DROP TABLE IF EXISTS twitter_sentiment;\")\n",
    "                \n",
    "                create_table_sql = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS twitter_sentiment (\n",
    "                    id LONG,\n",
    "                    original_tweet STRING,\n",
    "                    cleaned_tweet STRING,\n",
    "                    sentiment_label STRING,\n",
    "                    sentiment_score DOUBLE,\n",
    "                    confidence_score DOUBLE,\n",
    "                    word_count INT,\n",
    "                    char_count INT,\n",
    "                    processing_time_ms LONG,\n",
    "                    created_at TIMESTAMP,\n",
    "                    processed_at TIMESTAMP\n",
    "                ) TIMESTAMP(processed_at) PARTITION BY DAY;\n",
    "                \"\"\"\n",
    "                cur.execute(create_table_sql)\n",
    "                conn.commit()\n",
    "                logger.info(\"‚úÖ QuestDB table created/verified successfully\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating QuestDB table: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            return False\n",
    "        finally:\n",
    "            self.connection_pool.putconn(conn)\n",
    "\n",
    "    def clean_tweet(self, text: str) -> str:\n",
    "        \"\"\"Enhanced tweet cleaning function\"\"\"\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove user mentions and hashtags (but keep the content)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)  # Keep hashtag content\n",
    "        \n",
    "        # Remove special characters but keep punctuation that affects sentiment\n",
    "        text = re.sub(r'[^\\w\\s!?.,]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove very short tweets (less than 3 characters)\n",
    "        if len(text.strip()) < 3:\n",
    "            return \"\"\n",
    "            \n",
    "        return text.strip()\n",
    "\n",
    "    def get_enhanced_sentiment(self, text: str) -> Tuple[str, float, float]:\n",
    "        \"\"\"Enhanced sentiment analysis with confidence scoring\"\"\"\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return 'neutral', 0.0, 0.0\n",
    "        \n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            polarity = blob.sentiment.polarity\n",
    "            subjectivity = blob.sentiment.subjectivity\n",
    "            \n",
    "            # More nuanced sentiment classification\n",
    "            if polarity > 0.1:\n",
    "                label = 'positive'\n",
    "                confidence = min(polarity * 2, 1.0)  # Scale confidence\n",
    "            elif polarity < -0.1:\n",
    "                label = 'negative'\n",
    "                confidence = min(abs(polarity) * 2, 1.0)\n",
    "            else:\n",
    "                label = 'neutral'\n",
    "                confidence = 1.0 - abs(polarity)\n",
    "            \n",
    "            return label, polarity, confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in sentiment analysis for text: '{text[:50]}...': {e}\")\n",
    "            return 'neutral', 0.0, 0.0\n",
    "\n",
    "    def process_tweet_batch(self, tweet_batch: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process a batch of tweets for sentiment analysis\"\"\"\n",
    "        processed_tweets = []\n",
    "        \n",
    "        for tweet_data in tweet_batch:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Clean the tweet\n",
    "                original_tweet = tweet_data['tweet']\n",
    "                cleaned_tweet = self.clean_tweet(original_tweet)\n",
    "                \n",
    "                # Skip if cleaning resulted in empty tweet\n",
    "                if not cleaned_tweet:\n",
    "                    continue\n",
    "                \n",
    "                # Get sentiment analysis\n",
    "                sentiment_label, sentiment_score, confidence_score = self.get_enhanced_sentiment(cleaned_tweet)\n",
    "                \n",
    "                # Calculate processing time\n",
    "                processing_time_ms = int((time.time() - start_time) * 1000)\n",
    "                \n",
    "                # Prepare data for insertion\n",
    "                processed_tweet = {\n",
    "                    'id': tweet_data['id'],\n",
    "                    'original_tweet': original_tweet,\n",
    "                    'cleaned_tweet': cleaned_tweet,\n",
    "                    'sentiment_label': sentiment_label,\n",
    "                    'sentiment_score': sentiment_score,\n",
    "                    'confidence_score': confidence_score,\n",
    "                    'word_count': len(cleaned_tweet.split()),\n",
    "                    'char_count': len(cleaned_tweet),\n",
    "                    'processing_time_ms': processing_time_ms,\n",
    "                    'created_at': datetime.now(),\n",
    "                    'processed_at': datetime.now()\n",
    "                }\n",
    "                \n",
    "                processed_tweets.append(processed_tweet)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing tweet {tweet_data.get('id', 'unknown')}: {e}\")\n",
    "                self.failed_count += 1\n",
    "                continue\n",
    "        \n",
    "        return processed_tweets\n",
    "\n",
    "    def batch_insert_to_questdb(self, processed_tweets: List[Dict]) -> bool:\n",
    "        \"\"\"Batch insert tweets into QuestDB for better performance\"\"\"\n",
    "        if not processed_tweets:\n",
    "            return True\n",
    "            \n",
    "        conn = self.connection_pool.getconn()\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                # Prepare batch insert data\n",
    "                insert_data = []\n",
    "                for tweet in processed_tweets:\n",
    "                    insert_data.append((\n",
    "                        tweet['id'],\n",
    "                        tweet['original_tweet'],\n",
    "                        tweet['cleaned_tweet'],\n",
    "                        tweet['sentiment_label'],\n",
    "                        tweet['sentiment_score'],\n",
    "                        tweet['confidence_score'],\n",
    "                        tweet['word_count'],\n",
    "                        tweet['char_count'],\n",
    "                        tweet['processing_time_ms'],\n",
    "                        tweet['created_at'],\n",
    "                        tweet['processed_at']\n",
    "                    ))\n",
    "                \n",
    "                # Batch insert using execute_values for better performance\n",
    "                insert_sql = \"\"\"\n",
    "                INSERT INTO twitter_sentiment \n",
    "                (id, original_tweet, cleaned_tweet, sentiment_label, sentiment_score, \n",
    "                 confidence_score, word_count, char_count, processing_time_ms, created_at, processed_at)\n",
    "                VALUES %s\n",
    "                \"\"\"\n",
    "                \n",
    "                psycopg2.extras.execute_values(\n",
    "                    cur, insert_sql, insert_data,\n",
    "                    template=None, page_size=1000\n",
    "                )\n",
    "                \n",
    "                conn.commit()\n",
    "                self.processed_count += len(processed_tweets)\n",
    "                \n",
    "                logger.info(f\"Batch inserted {len(processed_tweets)} tweets. Total: {self.processed_count}\")\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in batch insert: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            conn.rollback()\n",
    "            return False\n",
    "        finally:\n",
    "            self.connection_pool.putconn(conn)\n",
    "\n",
    "    def process_dataset(self, filename: str, sample_size: Optional[int] = None) -> bool:\n",
    "        \"\"\"Process the entire dataset with batching and parallel processing\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"üìñ Loading dataset: {filename}\")\n",
    "            \n",
    "            # Load dataset with different encoding options\n",
    "            encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "            df = None\n",
    "            \n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    df = pd.read_csv(filename, encoding=encoding)\n",
    "                    logger.info(f\"Dataset loaded successfully with {encoding} encoding\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                logger.error(\"Could not load dataset with any encoding\")\n",
    "                return False\n",
    "            \n",
    "            # Data validation and cleaning\n",
    "            if 'tweet' not in df.columns:\n",
    "                # Try to find the tweet column\n",
    "                text_columns = [col for col in df.columns if 'text' in col.lower() or 'tweet' in col.lower()]\n",
    "                if text_columns:\n",
    "                    df = df.rename(columns={text_columns[0]: 'tweet'})\n",
    "                else:\n",
    "                    logger.error(\"No tweet column found in dataset\")\n",
    "                    return False\n",
    "            \n",
    "            # Remove null tweets\n",
    "            df = df.dropna(subset=['tweet'])\n",
    "            df = df[df['tweet'].astype(str).str.strip() != '']\n",
    "            \n",
    "            # Sample if requested\n",
    "            if sample_size and len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "                logger.info(f\"Using sample of {sample_size} tweets\")\n",
    "            \n",
    "            # Add ID column\n",
    "            df['id'] = range(1, len(df) + 1)\n",
    "            \n",
    "            logger.info(f\"Dataset info:\")\n",
    "            logger.info(f\"Total tweets: {len(df)}\")\n",
    "            logger.info(f\"Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Start processing\n",
    "            self.start_time = time.time()\n",
    "            self.processed_count = 0\n",
    "            self.failed_count = 0\n",
    "            \n",
    "            # Process in batches\n",
    "            total_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "            \n",
    "            for batch_idx in range(0, len(df), BATCH_SIZE):\n",
    "                batch_df = df.iloc[batch_idx:batch_idx + BATCH_SIZE]\n",
    "                batch_num = (batch_idx // BATCH_SIZE) + 1\n",
    "                \n",
    "                logger.info(f\"Processing batch {batch_num}/{total_batches} ({len(batch_df)} tweets)\")\n",
    "                \n",
    "                # Convert batch to list of dictionaries\n",
    "                tweet_batch = batch_df.to_dict('records')\n",
    "                \n",
    "                # Process the batch\n",
    "                processed_tweets = self.process_tweet_batch(tweet_batch)\n",
    "                \n",
    "                # Insert to database\n",
    "                if processed_tweets:\n",
    "                    success = self.batch_insert_to_questdb(processed_tweets)\n",
    "                    if not success:\n",
    "                        logger.warning(f\"Failed to insert batch {batch_num}\")\n",
    "                \n",
    "                # Progress update\n",
    "                elapsed_time = time.time() - self.start_time\n",
    "                tweets_per_second = self.processed_count / elapsed_time if elapsed_time > 0 else 0\n",
    "                logger.info(f\"Progress: {self.processed_count}/{len(df)} tweets processed ({tweets_per_second:.1f}/sec)\")\n",
    "            \n",
    "            # Final statistics\n",
    "            self.print_final_statistics(len(df))\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing dataset: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            return False\n",
    "\n",
    "    def print_final_statistics(self, total_tweets: int):\n",
    "        \"\"\"Print final processing statistics\"\"\"\n",
    "        elapsed_time = time.time() - self.start_time if self.start_time else 0\n",
    "        \n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"PROCESSING COMPLETE - FINAL STATISTICS\")\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(f\"Successfully processed: {self.processed_count} tweets\")\n",
    "        logger.info(f\"Failed to process: {self.failed_count} tweets\")\n",
    "        logger.info(f\"Success rate: {(self.processed_count/total_tweets)*100:.1f}%\")\n",
    "        logger.info(f\"Total processing time: {elapsed_time:.2f} seconds\")\n",
    "        logger.info(f\"Average speed: {self.processed_count/elapsed_time:.1f} tweets/second\")\n",
    "        \n",
    "        # Get sentiment distribution from database\n",
    "        self.print_sentiment_distribution()\n",
    "\n",
    "    def print_sentiment_distribution(self):\n",
    "        \"\"\"Print sentiment distribution from database\"\"\"\n",
    "        conn = self.connection_pool.getconn()\n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"\"\"\n",
    "                    SELECT sentiment_label, COUNT(*) as count \n",
    "                    FROM twitter_sentiment \n",
    "                    GROUP BY sentiment_label \n",
    "                    ORDER BY count DESC\n",
    "                \"\"\")\n",
    "                results = cur.fetchall()\n",
    "                \n",
    "                if results:\n",
    "                    logger.info(\"Sentiment Distribution:\")\n",
    "                    for label, count in results:\n",
    "                        percentage = (count / self.processed_count) * 100 if self.processed_count > 0 else 0\n",
    "                        logger.info(f\"   {label.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not retrieve sentiment distribution: {e}\")\n",
    "        finally:\n",
    "            self.connection_pool.putconn(conn)\n",
    "\n",
    "    def run_complete_pipeline(self, url: str, filename: str, sample_size: Optional[int] = None):\n",
    "        \"\"\"Run the complete sentiment analysis pipeline\"\"\"\n",
    "        logger.info(\"üöÄ Starting Twitter Sentiment Analysis Pipeline\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Download dataset\n",
    "            if not os.path.exists(filename):\n",
    "                if not self.download_dataset(url, filename):\n",
    "                    logger.error(\"Failed to download dataset\")\n",
    "                    return False\n",
    "            else:\n",
    "                logger.info(f\"üìÅ Using existing dataset: {filename}\")\n",
    "            \n",
    "            # Step 2: Create database table\n",
    "            if not self.create_questdb_table():\n",
    "                logger.error(\"Failed to create database table\")\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Process dataset\n",
    "            if not self.process_dataset(filename, sample_size):\n",
    "                logger.error(\"Failed to process dataset\")\n",
    "                return False\n",
    "            \n",
    "            logger.info(\"Pipeline completed successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Pipeline interrupted by user\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed with error: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b917745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Configuration\n",
    "    url = \"https://raw.githubusercontent.com/sharmaroshan/Twitter-Sentiment-Analysis/refs/heads/master/train_tweet.csv\"\n",
    "    filename = \"twitter_dataset.csv\"\n",
    "    sample_size = 10000  # Process only 10K tweets for testing (set to None for full dataset)\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = TwitterSentimentProcessor()\n",
    "    \n",
    "    try:\n",
    "        # Run the complete pipeline\n",
    "        success = processor.run_complete_pipeline(url, filename, sample_size)\n",
    "        \n",
    "        if success:\n",
    "            logger.info(\"All operations completed successfully!\")\n",
    "        else:\n",
    "            logger.error(\"Pipeline failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
